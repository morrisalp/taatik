{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learn transliteration.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morrisalp/taatik/blob/master/Learn_transliteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sozkkU-L8pxT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io\n",
        "import re\n",
        "import unicodedata\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bdY7PRBP9hCU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
      ]
    },
    {
      "metadata": {
        "id": "JxGxAmr84oyM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load and clean transliteration data from Wiktionary"
      ]
    },
    {
      "metadata": {
        "id": "d5oMK4G99CXR",
        "colab_type": "code",
        "outputId": "4f1e82ff-fb12-448b-faff-2519691797af",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "uploaded = files.upload() # upload hebrew_wiki_translit.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3467aa34-82f4-46ca-8e22-20a204679e78\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3467aa34-82f4-46ca-8e22-20a204679e78\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving hebrew_wiki_translit.csv to hebrew_wiki_translit.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KaAqzWzkUHRZ",
        "colab_type": "code",
        "outputId": "cef102e8-9610-4bfe-ae28-d08d7c8e5cd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# cleaning functions\n",
        "def normalize(text):\n",
        "  # normalizes e.g. order of multiple nikkud characters\n",
        "  out = unicodedata.normalize('NFC', text)\n",
        "  return out.replace(\n",
        "      '\\u05ba', '\\u05b9' # normalize holam\n",
        "  ).replace(\n",
        "      '\\u05bd', '' # remove meteg\n",
        "  ).replace(\n",
        "    '\\u05be', '-' # replace maqaf with hyphen\n",
        "  ).replace('״', '\"').replace(\"׳\", \"'\")\n",
        "open_brackets, close_brackets, vertical_bar, not_bracket = r'\\[\\[', r'\\]\\]', r'\\|', r'[^\\]]'\n",
        "link_regex = f'{open_brackets}(?:{not_bracket}*{vertical_bar})({not_bracket}*){close_brackets}'\n",
        "def remove_links(text):\n",
        "  #    [[אדריכלות|אַדְרִיכָלוּת]] [[נוף|נוֹף]]\n",
        "  # goes to:\n",
        "  #    אַדְרִיכָלוּת נוֹף\n",
        "  out = re.sub(link_regex, r'\\1', text)\n",
        "  return re.sub(r'\\([^\\)]*\\)', '', re.sub(r'\\{\\{[^\\}]*\\}\\}', '', out))\n",
        "# load scraped transliterations and clean them\n",
        "df = pd.read_csv(io.BytesIO(uploaded['hebrew_wiki_translit.csv']), keep_default_na = False)\n",
        "df.word = df.word.str.replace('״', '\"').str.replace(\"׳\", \"'\").str.strip()\n",
        "df.nikkud = df.nikkud.apply(normalize).apply(remove_links).str.strip()\n",
        "df.transliteration = df.transliteration.str.lower().str.replace(\"[׳\\u200f]\", \"'\").str.strip()\n",
        "# split multiwords\n",
        "n_word, n_nikkud, n_translit = [x.str.split().apply(len) for x in [df.word, df.nikkud, df.transliteration]]\n",
        "df = df[(n_word == n_nikkud) & (n_nikkud == n_translit)]\n",
        "df = pd.concat(list(\n",
        "    pd.DataFrame({\n",
        "        'word': t.word.split(),\n",
        "        'nikkud': t.nikkud.split(),\n",
        "        'transliteration': t.transliteration.split()\n",
        "    })\n",
        "    for t in tqdm(df.itertuples(), desc = 'Splitting multiwords', total = df.shape[0])\n",
        "))\n",
        "# remove bad characters\n",
        "df = df[\n",
        "    df.word.str.match('[א-ת]') &\n",
        "    ~df.word.str.contains(\"[^א-ת '\\\"]\") &\n",
        "    (df.nikkud != '') &\n",
        "    ~df.nikkud.str.contains(r'[^\\u0590-\\u05ff \\'\"]') &\n",
        "    ~df.nikkud.str.match('^[א-ת \\'\"]*$') &\n",
        "    df.transliteration.str.contains('[a-z]') &\n",
        "    ~df.transliteration.str.contains('[^a-z \\'\"]')\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting multiwords: 100%|██████████| 11922/11922 [00:08<00:00, 1478.94it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5KoMQOBbo1Hn",
        "colab_type": "code",
        "outputId": "91cbf7f1-46f0-4210-eef7-b49d1c41b3f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15490, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "ZT00MuIU4xTi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Make data sequences"
      ]
    },
    {
      "metadata": {
        "id": "PAo9o9v441QY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nikkud_maxlen = df.nikkud.apply(len).max()\n",
        "translit_maxlen = df.transliteration.apply(len).max()\n",
        "symbols = '^$ '\n",
        "nikkud_unique_chars = set(''.join(df.nikkud)) - set(symbols)\n",
        "translit_unique_chars = set(''.join(df.transliteration)) - set(symbols)\n",
        "nikkud_charset = symbols + ''.join(sorted(nikkud_unique_chars))\n",
        "translit_charset = symbols + ''.join(sorted(translit_unique_chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tqsgbpb646tI",
        "colab_type": "code",
        "outputId": "b8b406fc-13bf-423f-ffd9-17e5ea1e803a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(nikkud_maxlen, len(nikkud_charset), nikkud_charset)\n",
        "print(translit_maxlen, len(translit_charset), translit_charset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31 46 ^$ \"'ְֱֲֳִֵֶַָֹֻּׁׂאבגדהוזחטיךכלםמןנסעףפץצקרשת\n",
            "25 31 ^$ \"'abcdefghijklmnopqrstuvwxyz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kU7ETihB47jo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_word(word, pad_length):\n",
        "  return '^' + word + ' ' * (pad_length - len(word)) + '$'\n",
        "def word2onehot(word, charset, pad_length):\n",
        "  return tf.keras.utils.to_categorical([charset.index(c) for c in pad_word(word, pad_length)], num_classes = len(charset))\n",
        "def nikkud2onehot(nikkud):\n",
        "  return word2onehot(nikkud, nikkud_charset, nikkud_maxlen)\n",
        "def translit2onehot(translit):\n",
        "  return word2onehot(translit, translit_charset, translit_maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rKM2PhLH5AhB",
        "colab_type": "code",
        "outputId": "861b5263-d4f4-448c-f18f-b376119486f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "nikkud2onehot('צִיתָר')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "I3FH4TLJ5A3p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = np.array([nikkud2onehot(nikkud) for nikkud in df.nikkud])\n",
        "Y = np.array([translit2onehot(translit) for translit in df.transliteration])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9BqHD4c4-QBo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# teacher forcing - Z[:, t, :] = Y[:, t + 1, :]\n",
        "Z = np.roll(Y, -1, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cm8i49EX-Ma4",
        "colab_type": "code",
        "outputId": "d6eea4a1-6fe6-4904-d36b-0275fa7c0a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X.shape, Y.shape, Z.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((15490, 33, 46), (15490, 27, 31), (15490, 27, 31))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "ZloVRKi1-5S8",
        "colab_type": "code",
        "outputId": "630ab4c9-1981-4f25-e98f-84452b391415",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "(Z[:, 10, :] == Y[:, 11, :]).all()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "luIgArzJ_nIF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def onehot2word(onehot, charset):\n",
        "  return ''.join([charset[np.argmax(v)] for v in onehot])\n",
        "def onehot2nikkud(onehot):\n",
        "  return onehot2word(onehot, nikkud_charset)\n",
        "def onehot2translit(onehot):\n",
        "  return onehot2word(onehot, translit_charset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rYpBF6kB_3KD",
        "colab_type": "code",
        "outputId": "a709663f-a18f-44f8-a9d4-91d8e2c125ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "onehot2translit(Y[1, :, :])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'^eugenika                 $'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "5aUi49Ap_9de",
        "colab_type": "code",
        "outputId": "f6e3566a-fec8-4211-8104-8ba7944f4c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "onehot2translit(Z[1, :, :])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eugenika                 $^'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "YgIFYhElAEPQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Build seq2seq nikkud=>translit (N2T) model"
      ]
    },
    {
      "metadata": {
        "id": "JkZHfHyf7bdr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "latent_dim = 256\n",
        "batch_size = 256#128#64\n",
        "epochs = 100\n",
        "validation_split = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QblETpro5apZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = tf.keras.layers.Input(shape = (None, len(nikkud_charset)))#(None, num_encoder_tokens))\n",
        "encoder = tf.keras.layers.LSTM(latent_dim, return_state = True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IBeAWG8y7k3O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = tf.keras.layers.Input(shape = (None, len(translit_charset)))#(None, num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the \n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences = True, return_state = True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state = encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(len(translit_charset), activation = 'softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fbbBL1Wi7jDY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KLjPmpX06y86",
        "colab_type": "code",
        "outputId": "34427340-c797-4fd8-f26b-4fcbf7b802be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_31 (InputLayer)           (None, None, 46)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_32 (InputLayer)           (None, None, 31)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_14 (LSTM)                  [(None, 256), (None, 310272      input_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_15 (LSTM)                  [(None, None, 256),  294912      input_32[0][0]                   \n",
            "                                                                 lstm_14[0][1]                    \n",
            "                                                                 lstm_14[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, None, 31)     7967        lstm_15[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 613,151\n",
            "Trainable params: 613,151\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gkXR40bhEfnQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train N2T model"
      ]
    },
    {
      "metadata": {
        "id": "EOAUCxzE6wZL",
        "colab_type": "code",
        "outputId": "04413259-0d5f-4f07-f37e-312006357093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3451
        }
      },
      "cell_type": "code",
      "source": [
        "# Run training\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')\n",
        "model.fit([X, Y], Z,\n",
        "          batch_size = batch_size,\n",
        "          epochs = epochs,\n",
        "          validation_split = validation_split)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12392 samples, validate on 3098 samples\n",
            "Epoch 1/100\n",
            "12392/12392 [==============================] - 14s 1ms/step - loss: 1.2124 - val_loss: 0.9423\n",
            "Epoch 2/100\n",
            "12392/12392 [==============================] - 9s 687us/step - loss: 0.9517 - val_loss: 0.8828\n",
            "Epoch 3/100\n",
            "12392/12392 [==============================] - 9s 689us/step - loss: 0.8826 - val_loss: 0.8080\n",
            "Epoch 4/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.8093 - val_loss: 0.7773\n",
            "Epoch 5/100\n",
            "12392/12392 [==============================] - 9s 691us/step - loss: 0.7537 - val_loss: 0.7734\n",
            "Epoch 6/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.7162 - val_loss: 0.6815\n",
            "Epoch 7/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.6926 - val_loss: 0.6981\n",
            "Epoch 8/100\n",
            "12392/12392 [==============================] - 9s 698us/step - loss: 0.6783 - val_loss: 0.7311\n",
            "Epoch 9/100\n",
            "12392/12392 [==============================] - 9s 695us/step - loss: 0.6688 - val_loss: 0.6859\n",
            "Epoch 10/100\n",
            "12392/12392 [==============================] - 9s 695us/step - loss: 0.6596 - val_loss: 0.6461\n",
            "Epoch 11/100\n",
            "12392/12392 [==============================] - 9s 691us/step - loss: 0.6517 - val_loss: 0.6544\n",
            "Epoch 12/100\n",
            "12392/12392 [==============================] - 9s 696us/step - loss: 0.6475 - val_loss: 0.6406\n",
            "Epoch 13/100\n",
            "12392/12392 [==============================] - 9s 695us/step - loss: 0.6392 - val_loss: 0.6594\n",
            "Epoch 14/100\n",
            "12392/12392 [==============================] - 9s 689us/step - loss: 0.6333 - val_loss: 0.6511\n",
            "Epoch 15/100\n",
            "12392/12392 [==============================] - 9s 695us/step - loss: 0.6303 - val_loss: 0.6769\n",
            "Epoch 16/100\n",
            "12392/12392 [==============================] - 9s 690us/step - loss: 0.6250 - val_loss: 0.6630\n",
            "Epoch 17/100\n",
            "12392/12392 [==============================] - 9s 695us/step - loss: 0.6090 - val_loss: 0.6449\n",
            "Epoch 18/100\n",
            "12392/12392 [==============================] - 9s 700us/step - loss: 0.5995 - val_loss: 0.6017\n",
            "Epoch 19/100\n",
            "12392/12392 [==============================] - 9s 691us/step - loss: 0.5893 - val_loss: 0.6048\n",
            "Epoch 20/100\n",
            "12392/12392 [==============================] - 9s 691us/step - loss: 0.5762 - val_loss: 0.5884\n",
            "Epoch 21/100\n",
            "12392/12392 [==============================] - 9s 696us/step - loss: 0.5547 - val_loss: 0.5721\n",
            "Epoch 22/100\n",
            "12392/12392 [==============================] - 9s 695us/step - loss: 0.5298 - val_loss: 0.6234\n",
            "Epoch 23/100\n",
            "12392/12392 [==============================] - 9s 699us/step - loss: 0.5071 - val_loss: 0.5021\n",
            "Epoch 24/100\n",
            "12392/12392 [==============================] - 9s 703us/step - loss: 0.4834 - val_loss: 0.4921\n",
            "Epoch 25/100\n",
            "12392/12392 [==============================] - 9s 696us/step - loss: 0.4610 - val_loss: 0.4942\n",
            "Epoch 26/100\n",
            "12392/12392 [==============================] - 9s 696us/step - loss: 0.4399 - val_loss: 0.4368\n",
            "Epoch 27/100\n",
            "12392/12392 [==============================] - 9s 696us/step - loss: 0.4198 - val_loss: 0.4391\n",
            "Epoch 28/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.3903 - val_loss: 0.3894\n",
            "Epoch 29/100\n",
            "12392/12392 [==============================] - 9s 690us/step - loss: 0.3775 - val_loss: 0.7927\n",
            "Epoch 30/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.3706 - val_loss: 0.4559\n",
            "Epoch 31/100\n",
            "12392/12392 [==============================] - 9s 697us/step - loss: 0.3336 - val_loss: 0.3375\n",
            "Epoch 32/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.3116 - val_loss: 0.3123\n",
            "Epoch 33/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.3015 - val_loss: 0.2967\n",
            "Epoch 34/100\n",
            "12392/12392 [==============================] - 9s 691us/step - loss: 0.2837 - val_loss: 0.2937\n",
            "Epoch 35/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.2667 - val_loss: 0.2782\n",
            "Epoch 36/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.2462 - val_loss: 0.2470\n",
            "Epoch 37/100\n",
            "12392/12392 [==============================] - 9s 688us/step - loss: 0.2386 - val_loss: 0.2502\n",
            "Epoch 38/100\n",
            "12392/12392 [==============================] - 9s 694us/step - loss: 0.2281 - val_loss: 0.2304\n",
            "Epoch 39/100\n",
            "12392/12392 [==============================] - 9s 695us/step - loss: 0.2196 - val_loss: 0.2146\n",
            "Epoch 40/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.1927 - val_loss: 0.2223\n",
            "Epoch 41/100\n",
            "12392/12392 [==============================] - 9s 696us/step - loss: 0.2048 - val_loss: 0.1921\n",
            "Epoch 42/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.1882 - val_loss: 0.2274\n",
            "Epoch 43/100\n",
            "12392/12392 [==============================] - 9s 690us/step - loss: 0.1563 - val_loss: 0.1769\n",
            "Epoch 44/100\n",
            "12392/12392 [==============================] - 9s 690us/step - loss: 0.1725 - val_loss: 0.1673\n",
            "Epoch 45/100\n",
            "12392/12392 [==============================] - 9s 694us/step - loss: 0.1677 - val_loss: 0.1727\n",
            "Epoch 46/100\n",
            "12392/12392 [==============================] - 9s 697us/step - loss: 0.1619 - val_loss: 0.2002\n",
            "Epoch 47/100\n",
            "12392/12392 [==============================] - 9s 691us/step - loss: 0.1323 - val_loss: 0.1600\n",
            "Epoch 48/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.1523 - val_loss: 0.2080\n",
            "Epoch 49/100\n",
            "12392/12392 [==============================] - 9s 690us/step - loss: 0.1249 - val_loss: 0.1372\n",
            "Epoch 50/100\n",
            "12392/12392 [==============================] - 9s 690us/step - loss: 0.1344 - val_loss: 0.1336\n",
            "Epoch 51/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.1298 - val_loss: 0.1305\n",
            "Epoch 52/100\n",
            "12392/12392 [==============================] - 9s 695us/step - loss: 0.1005 - val_loss: 0.1588\n",
            "Epoch 53/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.1197 - val_loss: 0.1222\n",
            "Epoch 54/100\n",
            "12392/12392 [==============================] - 9s 689us/step - loss: 0.1164 - val_loss: 0.2205\n",
            "Epoch 55/100\n",
            "12392/12392 [==============================] - 9s 687us/step - loss: 0.0912 - val_loss: 0.1133\n",
            "Epoch 56/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.1076 - val_loss: 0.1277\n",
            "Epoch 57/100\n",
            "12392/12392 [==============================] - 9s 690us/step - loss: 0.1198 - val_loss: 0.1405\n",
            "Epoch 58/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.0960 - val_loss: 0.1091\n",
            "Epoch 59/100\n",
            "12392/12392 [==============================] - 9s 695us/step - loss: 0.1005 - val_loss: 0.1204\n",
            "Epoch 60/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.0745 - val_loss: 0.6921\n",
            "Epoch 61/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.0945 - val_loss: 0.1345\n",
            "Epoch 62/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.0905 - val_loss: 0.1085\n",
            "Epoch 63/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.0909 - val_loss: 0.1051\n",
            "Epoch 64/100\n",
            "12392/12392 [==============================] - 9s 688us/step - loss: 0.0629 - val_loss: 0.1134\n",
            "Epoch 65/100\n",
            "12392/12392 [==============================] - 9s 691us/step - loss: 0.0857 - val_loss: 0.1189\n",
            "Epoch 66/100\n",
            "12392/12392 [==============================] - 9s 694us/step - loss: 0.0826 - val_loss: 0.0994\n",
            "Epoch 67/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.0558 - val_loss: 0.1017\n",
            "Epoch 68/100\n",
            "12392/12392 [==============================] - 9s 694us/step - loss: 0.0955 - val_loss: 0.1026\n",
            "Epoch 69/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.0531 - val_loss: 0.1250\n",
            "Epoch 70/100\n",
            "12392/12392 [==============================] - 9s 690us/step - loss: 0.0799 - val_loss: 0.1327\n",
            "Epoch 71/100\n",
            "12392/12392 [==============================] - 9s 696us/step - loss: 0.0581 - val_loss: 0.1162\n",
            "Epoch 72/100\n",
            "12392/12392 [==============================] - 9s 691us/step - loss: 0.0754 - val_loss: 0.0989\n",
            "Epoch 73/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.0492 - val_loss: 0.3675\n",
            "Epoch 74/100\n",
            "12392/12392 [==============================] - 9s 686us/step - loss: 0.0582 - val_loss: 0.1027\n",
            "Epoch 75/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.0710 - val_loss: 0.1005\n",
            "Epoch 76/100\n",
            "12392/12392 [==============================] - 9s 694us/step - loss: 0.0408 - val_loss: 0.1178\n",
            "Epoch 77/100\n",
            "12392/12392 [==============================] - 9s 695us/step - loss: 0.0616 - val_loss: 0.1038\n",
            "Epoch 78/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.0600 - val_loss: 0.0957\n",
            "Epoch 79/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.0581 - val_loss: 0.1984\n",
            "Epoch 80/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.0386 - val_loss: 0.1050\n",
            "Epoch 81/100\n",
            "12392/12392 [==============================] - 9s 694us/step - loss: 0.0548 - val_loss: 0.0999\n",
            "Epoch 82/100\n",
            "12392/12392 [==============================] - 9s 694us/step - loss: 0.0362 - val_loss: 0.5467\n",
            "Epoch 83/100\n",
            "12392/12392 [==============================] - 9s 696us/step - loss: 0.0544 - val_loss: 0.1565\n",
            "Epoch 84/100\n",
            "12392/12392 [==============================] - 9s 690us/step - loss: 0.0319 - val_loss: 0.1048\n",
            "Epoch 85/100\n",
            "12392/12392 [==============================] - 9s 687us/step - loss: 0.0508 - val_loss: 0.1019\n",
            "Epoch 86/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.0491 - val_loss: 0.1033\n",
            "Epoch 87/100\n",
            "12392/12392 [==============================] - 9s 689us/step - loss: 0.0440 - val_loss: 0.1026\n",
            "Epoch 88/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.0431 - val_loss: 0.1012\n",
            "Epoch 89/100\n",
            "12392/12392 [==============================] - 9s 690us/step - loss: 0.0475 - val_loss: 0.1069\n",
            "Epoch 90/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.0440 - val_loss: 0.1132\n",
            "Epoch 91/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.0251 - val_loss: 0.1107\n",
            "Epoch 92/100\n",
            "12392/12392 [==============================] - 9s 689us/step - loss: 0.0484 - val_loss: 0.1038\n",
            "Epoch 93/100\n",
            "12392/12392 [==============================] - 9s 688us/step - loss: 0.0419 - val_loss: 0.1265\n",
            "Epoch 94/100\n",
            "12392/12392 [==============================] - 9s 696us/step - loss: 0.0278 - val_loss: 0.1144\n",
            "Epoch 95/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.0422 - val_loss: 0.1121\n",
            "Epoch 96/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.0352 - val_loss: 0.1068\n",
            "Epoch 97/100\n",
            "12392/12392 [==============================] - 9s 693us/step - loss: 0.0334 - val_loss: 0.1181\n",
            "Epoch 98/100\n",
            "12392/12392 [==============================] - 9s 692us/step - loss: 0.0392 - val_loss: 0.1058\n",
            "Epoch 99/100\n",
            "12392/12392 [==============================] - 9s 689us/step - loss: 0.0353 - val_loss: 0.1078\n",
            "Epoch 100/100\n",
            "12392/12392 [==============================] - 9s 687us/step - loss: 0.0317 - val_loss: 0.1618\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7e3efe47f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "metadata": {
        "id": "M6W34-RuGsLB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Build N2T inference model"
      ]
    },
    {
      "metadata": {
        "id": "BQ0o2pMGGL8u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = tf.keras.layers.Input(shape = (latent_dim,))\n",
        "decoder_state_input_c = tf.keras.layers.Input(shape = (latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state = decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = tf.keras.models.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q_WbN26iGuXa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_text, input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, len(translit_charset)))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, 0] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        char_probabilities = {\n",
        "            c: p for c, p in zip(translit_charset, output_tokens[0, -1, :])\n",
        "        }\n",
        "        sampled_char = max(translit_charset, key = lambda c: char_probabilities[c])\n",
        "        sampled_token_index = translit_charset.index(sampled_char)\n",
        "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "#         sampled_char = translit_charset[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '$' or\n",
        "           len(decoded_sentence) > translit_maxlen):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, len(translit_charset)))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3GnyYg-qHyqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def n2t(nikkud):\n",
        "  input_text = normalize(nikkud)\n",
        "  N = nikkud2onehot(input_text)[None]\n",
        "  return decode_sequence(input_text, N).replace('$', '').strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0JrzeoIcElE8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test N2T predictions"
      ]
    },
    {
      "metadata": {
        "id": "CT2-kek-CPc4",
        "colab_type": "code",
        "outputId": "59b7e6c5-2705-4f3f-b01e-ae53e31333d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "n2t('שַׁחֶפֶת')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'shakhefet'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "metadata": {
        "id": "OtM9NdcVFyRl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df2 = df.sample(n = 20, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QfwanbYrH-Og",
        "colab_type": "code",
        "outputId": "751749cb-a7fe-4485-b2d4-a8f6588a80b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df2['n2t'] = df2.nikkud.progress_apply(n2t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:02<00:00,  9.04it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DSjMRbRM1v3k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df2['match'] = df2.transliteration == df2.n2t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NGB4REmY14Uy",
        "colab_type": "code",
        "outputId": "643284c4-a694-471e-b5df-8be78dd285b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df2['match'].sum() / df2.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "metadata": {
        "id": "DrIeXSKcH8o-",
        "colab_type": "code",
        "outputId": "2782e154-00d8-406a-c292-9033e41d7f7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "cell_type": "code",
      "source": [
        "df2[['nikkud', 'transliteration', 'n2t', 'match']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nikkud</th>\n",
              "      <th>transliteration</th>\n",
              "      <th>n2t</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>סֶקְסְטַנְט</td>\n",
              "      <td>sekstant</td>\n",
              "      <td>sextaneya</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>מֻגְלָה</td>\n",
              "      <td>mugla</td>\n",
              "      <td>mugla</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>מוֹדַעַת</td>\n",
              "      <td>moda'at</td>\n",
              "      <td>moda'at</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>בָּרִיא</td>\n",
              "      <td>bari</td>\n",
              "      <td>bari</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>שְׁמַרְחֹם</td>\n",
              "      <td>shmarkhom</td>\n",
              "      <td>shmarmon</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>מִסְפָּן</td>\n",
              "      <td>mispan</td>\n",
              "      <td>mispan</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>רִיבָה</td>\n",
              "      <td>riva</td>\n",
              "      <td>riva</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>אֳנִיַּת</td>\n",
              "      <td>oniyat</td>\n",
              "      <td>oniyat</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>מִסְחָרִית</td>\n",
              "      <td>miskharit</td>\n",
              "      <td>miskharit</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>מֵמֵס</td>\n",
              "      <td>memes</td>\n",
              "      <td>memes</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>צִבּוּרִי</td>\n",
              "      <td>tsiburi</td>\n",
              "      <td>tsiburi</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>קוֹנְסֶפְּצְיָה</td>\n",
              "      <td>konseptsya</td>\n",
              "      <td>konsefyast</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>קֶשֶׁר</td>\n",
              "      <td>kesher</td>\n",
              "      <td>kesher</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>אַפּוֹטְרוֹפּוֹס</td>\n",
              "      <td>apotropos</td>\n",
              "      <td>apotropos</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>מִשְׁנֶה</td>\n",
              "      <td>mishne</td>\n",
              "      <td>mishne</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>תּוֹפֵשׂ</td>\n",
              "      <td>tofes</td>\n",
              "      <td>tofesh</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>מִנְהָלִי</td>\n",
              "      <td>minhali</td>\n",
              "      <td>minhali</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>יֶרַח</td>\n",
              "      <td>yerah</td>\n",
              "      <td>yerakh</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>רוֹבַאי</td>\n",
              "      <td>rovai</td>\n",
              "      <td>rovaa</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>לוּחַ</td>\n",
              "      <td>lu'akh</td>\n",
              "      <td>lu'akh</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             nikkud transliteration         n2t  match\n",
              "0       סֶקְסְטַנְט        sekstant   sextaneya  False\n",
              "0           מֻגְלָה           mugla       mugla   True\n",
              "0          מוֹדַעַת         moda'at     moda'at   True\n",
              "0           בָּרִיא            bari        bari   True\n",
              "0        שְׁמַרְחֹם       shmarkhom    shmarmon  False\n",
              "0          מִסְפָּן          mispan      mispan   True\n",
              "0            רִיבָה            riva        riva   True\n",
              "0          אֳנִיַּת          oniyat      oniyat   True\n",
              "1        מִסְחָרִית       miskharit   miskharit   True\n",
              "0             מֵמֵס           memes       memes   True\n",
              "1         צִבּוּרִי         tsiburi     tsiburi   True\n",
              "0   קוֹנְסֶפְּצְיָה      konseptsya  konsefyast  False\n",
              "0            קֶשֶׁר          kesher      kesher   True\n",
              "0  אַפּוֹטְרוֹפּוֹס       apotropos   apotropos   True\n",
              "1          מִשְׁנֶה          mishne      mishne   True\n",
              "0          תּוֹפֵשׂ           tofes      tofesh  False\n",
              "1         מִנְהָלִי         minhali     minhali   True\n",
              "0             יֶרַח           yerah      yerakh  False\n",
              "0           רוֹבַאי           rovai       rovaa  False\n",
              "0             לוּחַ          lu'akh      lu'akh   True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "metadata": {
        "id": "qW7zCzNaIVmv",
        "colab_type": "code",
        "outputId": "bcbd7678-cf9c-44e0-f0b2-e4cfb779d057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "[(x, n2t(x)) for x in 'אבגדהוזחטיכךלמםנןסעפףצץקרשת']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('א', 'os'),\n",
              " ('ב', 've'),\n",
              " ('ג', 'go'),\n",
              " ('ד', 'du'),\n",
              " ('ה', 'ho'),\n",
              " ('ו', 'wa'),\n",
              " ('ז', 'zu'),\n",
              " ('ח', 'lek'),\n",
              " ('ט', 'to'),\n",
              " ('י', 'yu'),\n",
              " ('כ', 'ch'),\n",
              " ('ך', 'cho'),\n",
              " ('ל', 'lo'),\n",
              " ('מ', 'mo'),\n",
              " ('ם', 'mo'),\n",
              " ('נ', 'no'),\n",
              " ('ן', 'no'),\n",
              " ('ס', 'su'),\n",
              " ('ע', \"'o\"),\n",
              " ('פ', 'fo'),\n",
              " ('ף', 'fe'),\n",
              " ('צ', 'tsa'),\n",
              " ('ץ', 'tush'),\n",
              " ('ק', 'chu'),\n",
              " ('ר', 'rush'),\n",
              " ('ש', 'sh'),\n",
              " ('ת', 'to')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "metadata": {
        "id": "IX8baZxPW6gz",
        "colab_type": "code",
        "outputId": "2c7515fd-1eec-4902-8bb9-8befdee55897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "n2t('שלום')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"'olsm\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "metadata": {
        "id": "2n1veBm0XaHw",
        "colab_type": "code",
        "outputId": "55d9cb5f-4e0b-474c-88fe-589e7faf4cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "n2t('מוריס')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mursi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "metadata": {
        "id": "nXhANJYnT4ca",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}